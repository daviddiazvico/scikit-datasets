{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skdatasets import load\n",
    "from skdatasets.utils.scores import hypotheses_table, scores_table"
	"from skdatasets.utils.validation import classifier_scatter, metaparameter_plot" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_experiment_score(config, info=None):\n",
    "    repository = config['dataset']['repository']\n",
    "    dataset = config['dataset']['dataset']\n",
    "    predictor = config['estimator']['predictor']\n",
    "    score = np.nan\n",
    "    if info is not None:\n",
    "        score = info['score']['values'] if type(info['score']) == dict else info['score']\n",
    "    return repository, dataset, predictor, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_dataset(X, y=None):\n",
    "    n_patterns = len(X)\n",
    "    dimension = X.shape[1]\n",
    "    n_classes = class_ratios = None\n",
    "    if (y is not None) and (y.dtype.kind in ('b', 'u', 'i')):\n",
    "        counts = np.unique(y, return_counts=True)[1]\n",
    "        n_classes = len(counts)\n",
    "        class_ratios = np.max(counts) / np.min(counts)\n",
    "    return n_patterns, dimension, n_classes, class_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_table(datasets):\n",
    "    table = pd.DataFrame(columns=('n. patterns', 'dimension', 'n. classes', 'class ratios'))\n",
    "    for repository, dataset, X, y in datasets:\n",
    "        table.at[repository + ':' + dataset] = describe_dataset(X, y=y)\n",
    "    return table.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../.results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores, hypotheses and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame()\n",
    "stds = pd.DataFrame()\n",
    "for dirpath, dirnames, filenames in os.walk(folder):\n",
    "    try:\n",
    "        config = json.load(open(os.path.join(dirpath, 'config.json')))\n",
    "        info = json.load(open(os.path.join(dirpath, 'info.json')))\n",
    "        repository, dataset, predictor, score = parse_experiment_score(config, info=info)\n",
    "        scores.at[repository + ':' + dataset, predictor] = np.nanmean(score)\n",
    "        stds.at[repository + ':' + dataset, predictor] = np.nanstd(score)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_table(scores.index, scores.columns.values, scores.values, stds.values)\n",
    "# TODO: scores_table(scores, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses_table_ = hypotheses_table(scores.values, scores.columns.values, multitest='friedmanchisquare')\n",
    "# TODO: hypotheses_table(scores, multitest='friedmanchisquare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses_table_[0]\n",
    "# TODO: all in one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses_table_[1]\n",
    "# TODO: all in one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [row.split(':') for row in scores.index]\n",
    "datasets = [(r, d, *load(r, d, return_X_y=True)[:2]) for r, d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_table(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-parameter search and prediction scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = '1'\n",
    "param = 'classifier__C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(os.path.join(folder, experiment, 'config.json')))\n",
    "repository, dataset, predictor, _ = parse_experiment_score(config)\n",
    "info = json.load(open(os.path.join(folder, experiment, 'info.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = pickle.load(open(os.path.join(folder, experiment, 'estimator.pkl'), 'rb'))\n",
    "metaparameter_plot(estimator, param, '/tmp/' + repository + '-' + dataset + '-' + predictor + '-' + param + '.png')\n",
    "# TODO: metaparameter_plot(info, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = pickle.load(open(os.path.join(folder, experiment, 'estimator.pkl'), 'rb'))\n",
    "X, y, X_test, _, _, outer_cv = load(repository, dataset, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_test is not None:\n",
    "    classifier_scatter(X_test, estimator.predict(X_test), '/tmp/' + repository + '-' + dataset + '-' + estimator + '_scatter.png')\n",
    "# TODO:    classifier_scatter(X_test, estimator.predict(X_test))\n",
    "else:\n",
    "    preds = cross_val_predict(estimator, X, y=y)\n",
    "    classifier_scatter(X, cross_val_predict(estimator, X, y=y, cv=outer_cv), '/tmp/' + repository + '-' + dataset + '-' + estimator + '_scatter.png')\n",
    "# TODO:    classifier_scatter(X, cross_val_predict(estimator, X, y=y, cv=outer_cv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:experiments]",
   "language": "python",
   "name": "conda-env-experiments-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
