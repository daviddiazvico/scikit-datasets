{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kruskal, friedmanchisquare, mannwhitneyu, rankdata, wilcoxon\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "\n",
    "def scores_table(datasets, estimators, scores, stds=None,\n",
    "                 greater_is_better=True, method='average'):\n",
    "    \"\"\" Scores table.\n",
    "\n",
    "        Prints a table where each row represents a dataset and each column\n",
    "        represents an estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datasets: array-like\n",
    "                  List of dataset names.\n",
    "        estimators: array-like\n",
    "                    List of estimator names.\n",
    "        scores: array-like\n",
    "                Matrix of scores where each column represents a model.\n",
    "        stds: array_like, default=None\n",
    "              Matrix of standard deviations where each column represents a\n",
    "              model.\n",
    "        greater_is_better: boolean, default=True\n",
    "                           Whether a greater score is better (score) or worse\n",
    "                           (loss).\n",
    "        method: {'average', 'min', 'max', 'dense', 'ordinal'}, default='average'\n",
    "                Method used to solve ties.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        table: array-like\n",
    "               Table of mean and standard deviation of each estimator-dataset\n",
    "               pair. A ranking of estimators is also generated.\n",
    "    \"\"\"\n",
    "    ranks = np.asarray([rankdata(-m, method=method) if greater_is_better else rankdata(m, method=method) for m in scores])\n",
    "    table = pd.DataFrame(data=scores, index=datasets, columns=estimators)\n",
    "    for i, d in enumerate(datasets):\n",
    "        for j, e in enumerate(estimators):\n",
    "            table.loc[d, e] = '{0:.2f}'.format(scores[i, j])\n",
    "            if stds is not None:\n",
    "                table.loc[d, e] += ' Â±{0:.2f}'.format(stds[i, j])\n",
    "            table.loc[d, e] += ' ({0:.1f})'.format(ranks[i, j])\n",
    "    table.loc['rank mean'] = np.around(np.mean(ranks, axis=0), decimals=4)\n",
    "    return table\n",
    "\n",
    "\n",
    "def hypotheses_table(samples, models, alpha=0.05, multitest=None,\n",
    "                     test='wilcoxon', correction=None, multitest_args=dict(),\n",
    "                     test_args=dict()):\n",
    "    \"\"\" Hypotheses table.\n",
    "\n",
    "        Prints a hypothesis table with a selected test and correction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples: array-like\n",
    "                 Matrix of samples where each column represent a model.\n",
    "        models: array-like\n",
    "                Model names.\n",
    "        alpha: float in [0, 1], default=0.05\n",
    "               Significance level.\n",
    "        multitest: {'kruskal', 'friedmanchisquare'}\n",
    "                   default=None\n",
    "                   Ranking multitest used.\n",
    "        test: {'mannwhitneyu', 'wilcoxon'},\n",
    "              default='wilcoxon'\n",
    "              Ranking test used.\n",
    "        correction: {'bonferroni', 'sidak', 'holm-sidak', 'holm',\n",
    "                     'simes-hochberg', 'hommel', 'fdr_bh', 'fdr_by', 'fdr_tsbh',\n",
    "                     'fdr_tsbky'},\n",
    "              default=None\n",
    "              Method used to adjust the p-values.\n",
    "        multitest_args: dict\n",
    "                        Optional ranking test arguments.\n",
    "        test_args: dict\n",
    "                   Optional ranking test arguments.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        multitest_table: array-like\n",
    "                         Table of p-value and rejection/non-rejection for the\n",
    "                         multitest hypothesis.\n",
    "        test_table: array-like\n",
    "               Table of p-values and rejection/non-rejection for each test\n",
    "               hypothesis.\n",
    "    \"\"\"\n",
    "    versus = list(it.combinations(range(len(models)), 2))\n",
    "    comparisons = [models[vs[0]] + \" vs \" + models[vs[1]] for vs in versus]\n",
    "    multitests = {'kruskal': kruskal, 'friedmanchisquare': friedmanchisquare}\n",
    "    tests = {'mannwhitneyu': mannwhitneyu, 'wilcoxon': wilcoxon}\n",
    "    multitest_table = None\n",
    "    if multitest is not None:\n",
    "        multitest_table = pd.DataFrame(index=[multitest], columns=['p-value',\n",
    "                                                                   'Hypothesis'])\n",
    "        statistic, pvalue = multitests[multitest](*samples, **multitest_args)\n",
    "        reject = 'Rejected' if pvalue <= alpha else 'Not rejected'\n",
    "        multitest_table.loc[multitest] = ['{0:.2f}'.format(pvalue), reject]\n",
    "        if pvalue > alpha:\n",
    "            return multitest_table, None\n",
    "    pvalues = [tests[test](samples[:, vs[0]], samples[:, vs[1]], **test_args)[1] for vs in versus]\n",
    "    if correction is not None:\n",
    "        reject, pvalues, alphac_sidak, alphac_bonf = multipletests(pvalues,\n",
    "                                                                   alpha,\n",
    "                                                                   method=correction)\n",
    "    else:\n",
    "        reject = ['Rejected' if pvalue <= alpha else 'Not rejected' for pvalue in pvalues]\n",
    "    test_table = pd.DataFrame(index=comparisons, columns=['p-value',\n",
    "                                                          'Hypothesis'])\n",
    "    for i, d in enumerate(comparisons):\n",
    "        test_table.loc[d] = ['{0:.2f}'.format(pvalues[i]), reject[i]]\n",
    "    return multitest_table, test_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from skdatasets.utils._scores import scores_table, hypotheses_table\n",
    "\n",
    "\n",
    "datasets = ['a4a', 'a8a', 'combined', 'dna', 'ijcnn1', 'letter', 'pendigits',\n",
    "            'satimage', 'shuttle', 'usps', 'w7a', 'w8a']\n",
    "estimators = ['LogisticRegression', 'MLPClassifier0', 'MLPClassifier1',\n",
    "              'MLPClassifier2', 'MLPClassifier3', 'MLPClassifier4',\n",
    "              'MLPClassifier5']\n",
    "scores = np.asarray(((89.79, 89.78, 89.76, 89.88, 89.85, 89.91, 89.93),\n",
    "                     (90.73, 90.73, 90.73, 90.85, 90.83, 90.81, 90.80),\n",
    "                     (92.36, 92.31, 94.58, 94.82, 94.84, 94.92, 94.89),\n",
    "                     (99.28, 99.27, 99.28, 99.26, 99.27, 99.25, 99.25),\n",
    "                     (91.34, 91.34, 99.29, 99.33, 99.34, 99.53, 99.54),\n",
    "                     (98.07, 98.04, 99.94, 99.95, 99.96, 99.96, 99.95),\n",
    "                     (99.17, 99.08, 99.87, 99.87, 99.88, 99.90, 99.89),\n",
    "                     (96.67, 96.28, 98.84, 98.87, 98.90, 98.87, 98.92),\n",
    "                     (95.85, 92.83, 99.88, 99.93, 99.96, 99.98, 99.99),\n",
    "                     (99.12, 99.11, 99.65, 99.58, 99.58, 99.65, 99.60),\n",
    "                     (95.93, 95.40, 94.58, 96.31, 96.34, 96.58, 96.50),\n",
    "                     (95.80, 95.99, 95.35, 96.20, 96.22, 96.36, 96.71)))\n",
    "\n",
    "\n",
    "def test_scores_table():\n",
    "    \"\"\"Tests scores table.\"\"\"\n",
    "    scores_table(datasets, estimators, scores)\n",
    "    scores_table(datasets, estimators, scores, stds=scores/10.0)\n",
    "\n",
    "\n",
    "def test_hypotheses_table():\n",
    "    \"\"\"Tests hypotheses table.\"\"\"\n",
    "    for multitest in ('kruskal', 'friedmanchisquare', None):\n",
    "        for test in ('mannwhitneyu', 'wilcoxon'):\n",
    "            hypotheses_table(scores, estimators, multitest=multitest, test=test)\n",
    "            for correction in ('bonferroni', 'sidak', 'holm-sidak', 'holm',\n",
    "                               'simes-hochberg', 'hommel', 'fdr_bh', 'fdr_by',\n",
    "                               'fdr_tsbh', 'fdr_tsbky'):\n",
    "                hypotheses_table(scores, estimators, multitest=multitest,\n",
    "                                 test=test, correction=correction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
